# O que vamos investigar? 
# Nosso objetivo √© usar esses dados para construir modelos capazes de: 
# ‚úî Prever a produtividade da safra (tarefa de regress√£o); 
# ‚úî Classificar a safra como baixa, m√©dia ou alta (tarefa de classifica√ß√£o).

# Mas antes de aplicar modelos, precisamos entender bem: 
# 1. As vari√°veis dispon√≠veis; 
# 2. As rela√ß√µes entre elas; 
# 3. A motiva√ß√£o por tr√°s dessas previs√µes. 

# Etapa 1 ‚Äî Importa√ß√£o de bibliotecas
from sklearn.linear_model import LogisticRegression 
from sklearn.multiclass import OneVsRestClassifier 
from itertools import combinations 
from mpl_toolkits.mplot3d import Axes3D  
from sklearn.linear_model import LinearRegression, Ridge 
from sklearn.pipeline import make_pipeline 
from sklearn.metrics import mean_squared_error, r2_score 
from sklearn.decomposition import PCA 
from sklearn.model_selection import train_test_split 
from sklearn.preprocessing import StandardScaler 
from sklearn.compose import ColumnTransformer  
import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt 
import seaborn as sns 
from IPython.display import display
import random
from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, classification_report
from sklearn.preprocessing import label_binarize 
from sklearn.metrics import roc_curve, auc 
from itertools import cycle
from matplotlib.colors import ListedColormap

# Vari√°veis globais para armazenar dados e modelos
df = None
X = None
y = None
X_scaled = None
X_pca = None
df_PCA = None
scaler = None
pca_model = None
modelo_linear = None
modelo_ridge = None
modelo_pca = None
modelo_pca_ridge = None
X_class = None
y_class = None
X_class_scaled = None
X_class_pca = None
modelo_classico = None
modelo_pca_class = None

def carregar_dados():
    """Carrega e prepara os dados iniciais"""
    global df
    
    # Abrindo o Arquivo
    df = pd.read_csv("DataSet.csv", sep=';', decimal=',')

    # Renomeando Colunas
    df.rename(columns={ 
    'chuva_durante_flora√ß√£o_mm': 'chuva_flor', 
    'chuva_durante_colheita_mm': 'chuva_colheita', 
    'chuva_total_anual_mm': 'chuva_total', 
    'anomalia_chuva_flora√ß√£o_mm': 'anomalia_flor',
    'temperatura_m√©dia_flora√ß√£o_C': 'temp_flor', 
    'umidade_relativa_m√©dia_flora√ß√£o_%': 'umid_flor', 
    'evento_ENSO': 'ENSO', 
    'produtividade_kg_por_ha': 'produtividade', 
    'produtividade_safra': 'safra' 
    }, inplace=True) 

    # Transformando em escala fracionaria 
    df['umid_flor'] = df['umid_flor'] / 100 
    df.set_index('ano', inplace=True) 
    
    print("Dados carregados com sucesso!")
    print("\nPrimeiras linhas do DataFrame:")
    print(df.head())
    
    # Ver informa√ß√µes gerais do dataframe 
    print("\nInforma√ß√µes do DataFrame:")
    df.info() 
    
    # Verificar valores ausentes 
    print("\nValores ausentes por coluna:") 
    print(df.isnull().sum()) 
    
    # Resumo estat√≠stico 
    print("\nResumo estat√≠stico:")
    print(df.describe().T)

def criar_variaveis_adicionais():
    global df, df_raw

    # Salva o original antes das dummies
    df_raw = df.copy()

    # Cria vari√°veis adicionais no df_raw tamb√©m (para gr√°ficos)
    df_raw['chuva_relativa'] = df_raw['chuva_flor'] / df_raw['chuva_total'] 
    df_raw['anomalia_bin'] = (df_raw['anomalia_flor'] > 0).astype(int)

    # Agora no df (com dummies para an√°lise estat√≠stica/machine learning)
    df = pd.get_dummies(df, columns=['ENSO'], drop_first=True)
    df['chuva_relativa'] = df['chuva_flor'] / df['chuva_total'] 
    df['anomalia_bin'] = (df['anomalia_flor'] > 0).astype(int)


# Boxplot: ENSO √ó Produtividade
def bloxplot():
    global df_raw

    sns.set(style="whitegrid", palette="colorblind") 
    sns.boxplot( 
        data=df_raw, 
        x='ENSO', 
        y='produtividade', 
        order=['La Ni√±a', 'Neutro', 'El Ni√±o'] 
    ) 
    plt.title('Produtividade vs. Evento ENSO', fontsize=14) 
    plt.xlabel('Evento ENSO', fontsize=12) 
    plt.ylabel('Produtividade (kg/ha)', fontsize=12) 
    plt.xticks(fontsize=10) 
    plt.yticks(fontsize=10) 
    plt.tight_layout() 
    plt.show()

# Scatter: Temperatura x Produtividade
def scatterplot():

   

    sns.scatterplot(data=df, x='temp_flor', y='produtividade', \
                    hue='ENSO_La Ni√±a', s=80, alpha=0.8) 
    plt.title('Temperatura durante flora√ß√£o vs. Produtividade', fontsize=14) 
    plt.xlabel('Temperatura m√©dia durante flora√ß√£o (¬∞C)', fontsize=12) 
    plt.ylabel('Produtividade (kg/ha)', fontsize=12) 
    plt.legend(title='Evento ENSO') 
    plt.show()

# HistogramaDEVariaveisNumericas():
def HistogramaDeVariaveisNumericas():
    df.select_dtypes(include='number').hist(bins=15, figsize=(12,8)) 
    plt.suptitle("Distribui√ß√µes das vari√°veis num√©ricas") 
    plt.show() 

#Heatmap de Correla√ß√£o():
def HeatmapDeCorrelacao():
    # Seleciona s√≥ as colunas num√©ricas relevantes 
    variaveis_numericas = df.select_dtypes(include='number') 
    
    # Calcula a matriz de correla√ß√£o 
    correlacao = variaveis_numericas.corr() 
    # Heatmap 
    plt.figure(figsize=(10, 6)) 
    sns.heatmap( 
    correlacao, 
    annot=True, 
    fmt=".2f", 
    cmap='coolwarm', 
    linewidths=0.5, 
    square=True, 
    cbar_kws={"shrink": .8}, 
    vmin=-1, vmax=1 
    ) 
    plt.title('Matriz de Correla√ß√£o entre Vari√°veis Num√©ricas') 
    plt.tight_layout() 
    plt.show()

# Pairplot()
def pairplot():    
    # Seleciona as vari√°veis num√©ricas (sem o ano) 
    cols_plot = ['chuva_flor', 'chuva_colheita', 'chuva_total', 
    'anomalia_flor', 'temp_flor', 'umid_flor', 'produtividade'] 
    # Pairplot 
    sns.pairplot( 
    df[cols_plot], 
    corner=True, 
    # evita duplica√ß√£o acima/abaixo da diagonal 
    diag_kind='hist', # ou 'kde' 
    plot_kws={'alpha': 0.7, 's': 40, 'edgecolor': 'k'} 
    ) 
    plt.suptitle("Matriz de Dispers√£o entre Vari√°veis", fontsize=14, y=1.02) 
    plt.show()

def analise_exploratoria():
    """Executa todas as an√°lises explorat√≥rias"""
    print("\n=== AN√ÅLISE EXPLORAT√ìRIA DE DADOS ===\n")
    
    opcoes = {
        '1': ('Boxplot: ENSO √ó Produtividade', bloxplot),
        '2': ('Scatter: Temperatura √ó Produtividade', scatterplot),
        '3': ('Histograma de Vari√°veis Num√©ricas', HistogramaDeVariaveisNumericas),
        '4': ('Heatmap de Correla√ß√£o', HeatmapDeCorrelacao),
        '5': ('Pairplot', pairplot),
        '6': ('Executar todas as visualiza√ß√µes', None)
    }
    
    print("Escolha uma visualiza√ß√£o:")
    for key, (desc, _) in opcoes.items():
        print(f"{key}. {desc}")
    print("0. Voltar ao menu principal")
    
    escolha = input("\nDigite sua escolha: ")
    
    if escolha == '0':
        return
    elif escolha == '6':
        for key in ['1', '2', '3', '4', '5']:
            print(f"\nExecutando: {opcoes[key][0]}")
            opcoes[key][1]()
    elif escolha in opcoes:
        print(f"\nExecutando: {opcoes[escolha][0]}")
        opcoes[escolha][1]()
    else:
        print("Op√ß√£o inv√°lida!")

def preparar_dados_regressao():
    """Prepara os dados para modelos de regress√£o"""
    global X, y, X_train, X_test, y_train, y_test, preprocessador
    global colunas_numericas, colunas_binarias
    
    # 1. Definindo X e y 
    X = df.drop(columns=['produtividade', 'safra']) # safra √© para classifica√ß√£o 
    y = df['produtividade'] 

    # 2. Verificando colunas num√©ricas e bin√°rias
    # Lista de colunas num√©ricas
    colunas_numericas = ['chuva_flor', 'chuva_colheita', 'chuva_total', 
    'anomalia_flor', 'temp_flor', 'umid_flor', 'chuva_relativa']
    # Lista de colunas bin√°rias 
    colunas_binarias = ['anomalia_bin', 'ENSO_La Ni√±a', 'ENSO_Neutro'] 

    # 3. Criando o transformador
    preprocessador = ColumnTransformer(transformers=[('num', StandardScaler(), 
    colunas_numericas),('bin', 'passthrough', colunas_binarias)]) 

    # 4. Separando treino e teste sem embaralhar (respeitando ordem temporal) 
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)
    
    print("Dados preparados para regress√£o!")
    print(f"Tamanho do conjunto de treino: {len(X_train)}")
    print(f"Tamanho do conjunto de teste: {len(X_test)}")

# Avalia√ß√£o da vari√¢ncia explicada (Scree Plot)
def screeplot():
    global X_padronizado, pca_full, pca, X_pca, df_PCA
    
    # Aplica o ColumnTransformer (padroniza√ß√£o) 
    X_padronizado = preprocessador.fit_transform(X) 

    # Aplica PCA com todos os componentes (n√£o limita n_components ainda) 
    pca_full = PCA() 
    pca_full.fit(X_padronizado) 
    # Scree Plot 
    plt.plot(range(1, len(pca_full.explained_variance_ratio_)+1), 
    pca_full.explained_variance_ratio_, marker='o') 
    plt.title('Scree Plot - Vari√¢ncia Explicada por Componente') 
    plt.xlabel('Componente Principal') 
    plt.ylabel('Propor√ß√£o da Vari√¢ncia') 
    plt.grid(True) 
    plt.tight_layout() 
    plt.show() 
    # Mostrar numericamente 
    for i, v in enumerate(pca_full.explained_variance_ratio_): 
        print(f"PC{i+1}: {v:.2%}")
    # Aplica PCA com 2 componentes 
    pca = PCA(n_components=2) 
    X_pca = pca.fit_transform(X_padronizado) 
    # Cria df_PCA com componentes e vari√°veis-alvo 
    df_PCA = pd.DataFrame(X_pca, columns=['PC1', 'PC2'], index=X.index) 
    df_PCA['produtividade'] = df['produtividade'] 
    df_PCA['safra'] = df['safra'] 
    
    # Visualiza√ß√£o 2D dos componentes principais
    def plot_pca_2d():
        sns.scatterplot(data=df_PCA, x='PC1', y='PC2', hue='safra', s=80, alpha=0.8) 
        plt.title('PCA - Componentes Principais coloridos por Safra') 
        plt.xlabel('Componente Principal 1') 
        plt.ylabel('Componente Principal 2') 
        plt.legend(title='Safra') 
        plt.tight_layout() 
        plt.show() 
    plot_pca_2d()

def treinar_modelos_regressao():
    """Treina todos os modelos de regress√£o"""
    global pipeline_original, pipeline_ridge, X_pca, y_pca
    global X_pca_train, X_pca_test, y_pca_train, y_pca_test
    global modelo_pca, modelo_pca_ridge
    global y_pred_orig, y_pred_ridge, y_pred_pca, y_pred_pca_ridge
    
    print("\n=== TREINANDO MODELOS DE REGRESS√ÉO ===\n")
    
    # Pipeline: pr√©-processador + modelo 
    pipeline_original = make_pipeline(preprocessador, LinearRegression()) 
    # Treinamento 
    pipeline_original.fit(X_train, y_train) 
    # Previs√£o 
    y_pred_orig = pipeline_original.predict(X_test) 
    # Avalia√ß√£o 
    mse_orig = mean_squared_error(y_test, y_pred_orig) 
    rmse_orig = mse_orig ** 0.5 
    r2_orig = r2_score(y_test, y_pred_orig) 
    print(f"[Regress√£o linear] RMSE: {rmse_orig:.2f} | R¬≤: {r2_orig:.2%}")
    
    # Pipeline com regulariza√ß√£o L2 (Ridge) 
    lambda_regressao = 1 # testar v√°rios valores para lambda 
    pipeline_ridge = make_pipeline(preprocessador, Ridge(alpha=lambda_regressao)) 
    
    # Treinamento 
    pipeline_ridge.fit(X_train, y_train) 
    # Previs√£o 
    y_pred_ridge = pipeline_ridge.predict(X_test) 
    # Avalia√ß√£o 
    mse_ridge = mean_squared_error(y_test, y_pred_ridge) 
    rmse_ridge = mse_ridge ** 0.5 
    r2_ridge = r2_score(y_test, y_pred_ridge) 
    print(f"[Regulariza√ß√£o Ridge (L¬≤) | Œª = {lambda_regressao}] RMSE: {rmse_ridge:.2f} | R¬≤: {r2_ridge:.2%}")
    
    # Definindo X e y com base no df_PCA 
    X_pca = df_PCA[['PC1', 'PC2']] 
    y_pca = df_PCA['produtividade'] 
    # Divis√£o temporal (como fizemos antes) 
    X_pca_train, X_pca_test, y_pca_train, y_pca_test = train_test_split(X_pca, 
    y_pca, test_size=0.2, shuffle=False) 
    # Modelo linear simples com PCA 
    modelo_pca = LinearRegression() 
    modelo_pca.fit(X_pca_train, y_pca_train) 
    # Previs√£o 
    y_pred_pca = modelo_pca.predict(X_pca_test) 
    # Avalia√ß√£o 
    rmse_pca = mean_squared_error(y_pca_test, y_pred_pca) ** 0.5 
    r2_pca = r2_score(y_pca_test, y_pred_pca) 
    print(f"[PCA + Regress√£o linear] RMSE: {rmse_pca:.2f} | R¬≤: {r2_pca:.2%}")
    
    # Modelo com Ridge sobre PCA 
    lambda_regressao = 1 # testar v√°rios valores para lambda 
    modelo_pca_ridge = Ridge(alpha=lambda_regressao) 
    modelo_pca_ridge.fit(X_pca_train, y_pca_train) 
    # Previs√£o 
    y_pred_pca_ridge = modelo_pca_ridge.predict(X_pca_test) 
    # Avalia√ß√£o 
    rmse_pca_ridge = mean_squared_error(y_pca_test, y_pred_pca_ridge) ** 0.5 
    r2_pca_ridge = r2_score(y_pca_test, y_pred_pca_ridge) 
    print(f"[PCA + Regulariza√ß√£o Ridge (L¬≤) | Œª = {lambda_regressao}] RMSE: {rmse_pca_ridge:.2f} | R¬≤: {r2_pca_ridge:.2%}")

def visualizar_comparacao_lambda():
    """Visualiza a compara√ß√£o do RMSE em fun√ß√£o de Œª"""
    # Simula√ß√£o dos dados para plotagem 
    lambdas = [0.1, 1, 10, 100, 1000, 10000, 30000, 100000, 300000, 1000000] 
    rmse_sem_pca = [71.19, 68.30, 54.97, 34.91, 26.38, 25.61, 25.56, 25.55, 
    25.54, 25.54]   
    rmse_com_pca = [43.25, 42.98, 40.61, 31.20, 25.97, 25.57, 25.55, 25.54, 
    25.54, 25.54]   
    
    plt.plot(lambdas, rmse_sem_pca, marker='o', label='Sem PCA') 
    plt.plot(lambdas, rmse_com_pca, marker='s', label='Com PCA') 
    plt.xscale('log') 
    plt.xlabel("Œª (log scale)") 
    plt.ylabel("RMSE") 
    plt.title("Compara√ß√£o do RMSE em fun√ß√£o de Œª (Ridge)") 
    plt.grid(True) 
    plt.legend() 
    plt.tight_layout() 
    plt.show()

def plot_modelos_para_variavel(x_var, X, y, scaler, pca_model, modelo_linear, 
modelo_ridge, modelo_pca, modelo_pca_ridge): 
    x_index = X.columns.get_loc(x_var) 
    x_vals = np.linspace(X[x_var].min(), X[x_var].max(), 100) 
    X_mean = X.mean().to_numpy() 
    X_input = np.tile(X_mean, (100, 1)) 
    X_input[:, x_index] = x_vals 
    X_input_df = pd.DataFrame(X_input, columns=X.columns) # ‚¨Ö usa os mesmos nomes 
    X_input_scaled = scaler.transform(X_input_df) 
    X_input_pca = pca_model.transform(X_input_scaled) 
    y_linear = modelo_linear.predict(X_input_scaled) 
    y_ridge = modelo_ridge.predict(X_input_scaled) 
    y_pca = modelo_pca.predict(X_input_pca) 
    y_pca_ridge = modelo_pca_ridge.predict(X_input_pca) 
    plt.figure(figsize=(10, 6)) 
    sns.scatterplot(x=X[x_var], y=y, color='red', label='Dados reais', s=50, 
    edgecolor='black') 
    plt.plot(x_vals, y_linear, label='Linear', linestyle='-', color='blue') 
    plt.plot(x_vals, y_ridge, label='Ridge (Œª=1.000.000)', linestyle='--', 
    color='orange') 
    plt.plot(x_vals, y_pca, label='PCA + Linear', linestyle='-.', 
    color='green') 
    plt.plot(x_vals, y_pca_ridge, label='PCA + Ridge (Œª=100.000)', 
    linestyle=':', color='purple') 
    plt.xlabel(x_var) 
    plt.ylabel('Produtividade (kg/ha)') 
    plt.title(f'Compara√ß√£o de modelos ‚Äî {x_var}') 
    plt.legend() 
    plt.grid(True) 
    plt.tight_layout() 
    plt.show()

def preparar_e_visualizar_modelos():
    """Prepara dados e visualiza compara√ß√£o de modelos"""
    global X, y, scaler, X_scaled, pca_model, X_pca, df_pca
    global modelo_linear, modelo_ridge, modelo_pca, modelo_pca_ridge
    
    # 1. Reconstru√ß√£o de X e y 
    X = df[[ 
    'chuva_flor', 'chuva_colheita', 'chuva_total', 
    'anomalia_flor', 'temp_flor', 'umid_flor', 'chuva_relativa' 
    ]] 
    y = df['produtividade'] 
    # 2. Padroniza√ß√£o 
    scaler = StandardScaler() 
    X_scaled = scaler.fit_transform(X) 
    # 3. PCA 
    pca_model = PCA(n_components=2) 
    X_pca = pca_model.fit_transform(X_scaled) 
    # Converte o array PCA em DataFrame com nomes 
    df_pca = pd.DataFrame(X_pca, columns=['PC1', 'PC2'], index=df.index) 
    df[['PC1', 'PC2']] = df_pca 
    # 4. Modelos treinados separadamente 
    modelo_linear = LinearRegression().fit(X_scaled, y) 
    modelo_ridge = Ridge(alpha=1e6).fit(X_scaled, y) 
    modelo_pca = LinearRegression().fit(X_pca, y) 
    modelo_pca_ridge = Ridge(alpha=1e5).fit(X_pca, y) 
    
    plot_modelos_para_variavel('temp_flor', X, y, scaler, pca_model, 
    modelo_linear, modelo_ridge, modelo_pca, modelo_pca_ridge)

#### Curva 1D da fun√ß√£o custo 
def plot_funcao_custo_1D(x_var, X, y, intervalo=(-200, 200), pontos=200): 
# """ Plota a fun√ß√£o de custo J(Œ∏‚ÇÅ) para uma regress√£o univariada com a 
# vari√°vel x_var. """ 
    x = X[x_var].values 
    y = y.values
    m = len(y) 
# Centraliza x para eliminar o intercepto implicitamente 
    x_centralizado = x - x.mean() 
    theta1_vals = np.linspace(intervalo[0], intervalo[1], pontos) 
    custos = [(1 / (2 * m)) * np.sum((theta1 * x_centralizado - y) ** 2) for 
    theta1 in theta1_vals] 
    plt.figure(figsize=(8, 5)) 
    plt.plot(theta1_vals, custos) 
    plt.xlabel("Œ∏‚ÇÅ") 
    plt.ylabel("J(Œ∏‚ÇÅ)") 
    plt.title(f"Fun√ß√£o de Custo - {x_var} (x centralizado)") 
    plt.grid(True) 
    plt.tight_layout() 
    plt.show()

def plot_funcao_custo_2D(x_vars, X, y, range_theta=(-200, 200), pontos=100): 
# Plota a superf√≠cie da fun√ß√£o de custo J(Œ∏‚ÇÅ, Œ∏‚ÇÇ) para duas vari√°veis. 
    x1 = X[x_vars[0]].values 
    x2 = X[x_vars[1]].values 
    y = y.values 
    m = len(y) 
# Matriz de entrada com intercepto 
    X_mat = np.vstack([np.ones(m), x1, x2]).T 
# Gera√ß√£o de grid de Œ∏‚ÇÅ e Œ∏‚ÇÇ (intercepto Œ∏‚ÇÄ fixado em 0 para simplifica√ß√£o) 
    theta1_vals = np.linspace(range_theta[0], range_theta[1], pontos) 
    theta2_vals = np.linspace(range_theta[0], range_theta[1], pontos) 
    J_vals = np.zeros((pontos, pontos)) 
    for i in range(pontos): 
        for j in range(pontos): 
            theta = np.array([0, theta1_vals[i], theta2_vals[j]]) # Œ∏‚ÇÄ = 0 
            h = X_mat @ theta 
            J_vals[j, i] = (1 / (2 * m)) * np.sum((h - y) ** 2) 
# Superf√≠cie 
    T1, T2 = np.meshgrid(theta1_vals, theta2_vals) 
    fig = plt.figure(figsize=(10, 6)) 
    ax = fig.add_subplot(111, projection='3d') 
    ax.plot_surface(T1, T2, J_vals, cmap='viridis', edgecolor='none', 
    alpha=0.9) 
    ax.set_xlabel(f"Œ∏‚ÇÅ ({x_vars[0]})") 
    ax.set_ylabel(f"Œ∏‚ÇÇ ({x_vars[1]})") 
    ax.set_zlabel("J(Œ∏)") 
    ax.set_title(f"Superf√≠cie da Fun√ß√£o de Custo ‚Äî {x_vars[0]} e {x_vars[1]}") 
# plt.tight_layout() 
    fig.subplots_adjust(right=0.5) 
    plt.show()

def plot_funcao_custo_2D_PCA(X_pca, y, range_theta=(-200, 200), pontos=100):
    pc1 = X_pca[:, 0] 
    pc2 = X_pca[:, 1] 
    m = len(y) 
# Matriz de entrada com intercepto 
    X_mat = np.vstack([np.ones(m), pc1, pc2]).T 
# Grid de valores de Œ∏‚ÇÅ e Œ∏‚ÇÇ 
    theta1_vals = np.linspace(range_theta[0], range_theta[1], pontos) 
    theta2_vals = np.linspace(range_theta[0], range_theta[1], pontos) 
    J_vals = np.zeros((pontos, pontos)) 
    for i in range(pontos): 
        for j in range(pontos): 
            theta = np.array([0, theta1_vals[i], theta2_vals[j]]) 
            h = X_mat @ theta 
            J_vals[j, i] = (1 / (2 * m)) * np.sum((h - y) ** 2) 
# Superf√≠cie 3D 
    T1, T2 = np.meshgrid(theta1_vals, theta2_vals) 
    fig = plt.figure(figsize=(10, 6)) 
    ax = fig.add_subplot(111, projection='3d') 
    ax.plot_surface(T1, T2, J_vals, cmap='viridis', edgecolor='none', alpha=0.9) 
    ax.set_xlabel("Œ∏‚ÇÅ (PC1)") 
    ax.set_ylabel("Œ∏‚ÇÇ (PC2)") 
    ax.set_zlabel("J(Œ∏)") 
    ax.set_title("Superf√≠cie da Fun√ß√£o de Custo ‚Äî Componentes Principais (PCA)") 
    fig.subplots_adjust(right=0.85) 
    plt.show()

def plot_residuos(y_true, y_pred, titulo): 
    residuos = y_true - y_pred 
    plt.figure(figsize=(8, 4)) 
    plt.scatter(y_pred, residuos, color='royalblue', alpha=0.7) 
    plt.axhline(0, color='red', linestyle='--') 
    plt.xlabel("Previs√£o") 
    plt.ylabel("Res√≠duo") 
    plt.title(f"Res√≠duos ‚Äî {titulo}") 
    plt.grid(True) 
    plt.tight_layout() 
    plt.show() 
    print("\n")

def visualizar_residuos():
    """Visualiza os res√≠duos de todos os modelos"""
    global y_pred_linear, y_pred_ridge, y_pred_pca, y_pred_pca_ridge
    
    # Previs√µes 
    y_pred_linear = modelo_linear.predict(X_scaled) 
    y_pred_ridge = modelo_ridge.predict(X_scaled) 
    y_pred_pca = modelo_pca.predict(X_pca) 
    y_pred_pca_ridge = modelo_pca_ridge.predict(X_pca) 
    # Gr√°ficos de res√≠duos 
    plot_residuos(y, y_pred_linear, "Regress√£o Linear") 
    plot_residuos(y, y_pred_ridge, "Regress√£o Regularizada (Œª = 1.000.000)") 
    plot_residuos(y, y_pred_pca, "PCA + Regress√£o Linear") 
    plot_residuos(y, y_pred_pca_ridge, "PCA + Regularizada (Œª = 100.000)")

def preparar_dados_classificacao():
    """Prepara os dados para modelos de classifica√ß√£o"""
    global X_class, y_class, scaler_class, X_class_scaled, pca_class, X_class_pca
    global X_train_class, X_test_class, y_train_class, y_test_class
    global X_train_pca, X_test_pca
    
    # 1. Mapeia a vari√°vel alvo (safra)
    mapa_safra = {'baixa': 0, 'media': 1, 'alta': 2}
    df['safra_num'] = df['safra'].map(mapa_safra)

    # 2. Define vari√°veis preditoras (mesmas da regress√£o)
    X_class = df[['chuva_flor', 'chuva_colheita', 'chuva_total', 'anomalia_flor',
                 'temp_flor', 'umid_flor', 'chuva_relativa']]
    y_class = df['safra_num'].fillna(1)  # Substitui NaN por 1 (media) 

    # 3. Padroniza√ß√£o 
    scaler_class = StandardScaler() 
    X_class_scaled = scaler_class.fit_transform(X_class) 
    # 4. PCA (opcional ‚Äî ser√° usado para um dos modelos) 
    pca_class = PCA(n_components=2) 
    X_class_pca = pca_class.fit_transform(X_class_scaled) 
    # 5. Divis√£o treino/teste 
    X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(X_class_scaled, y_class, test_size=0.3, random_state=42,stratify=y_class) 
    X_train_pca, X_test_pca, _, _ = train_test_split(X_class_pca, y_class, test_size=0.3, random_state=42, stratify=y_class)
    
    print("Dados preparados para classifica√ß√£o!")
    print(f"Classes: {mapa_safra}")
    print(f"Distribui√ß√£o das classes: \n{y_class.value_counts().sort_index()}")

def treinar_modelos_classificacao():
    """Treina os modelos de classifica√ß√£o"""
    global modelo_classico, modelo_pca_class
    
    # 1. Modelo com vari√°veis originais 
    modelo_classico = OneVsRestClassifier(LogisticRegression(max_iter=1000)) 
    modelo_classico.fit(X_train_class, y_train_class) 
    # 2. Modelo com PCA 
    modelo_pca_class = OneVsRestClassifier(LogisticRegression(max_iter=1000)) 
    modelo_pca_class.fit(X_train_pca, y_train_class)
    
    print("Modelos de classifica√ß√£o treinados!")

def plotar_curva_roc_multiclasse(y_true, y_score, classes, titulo="Modelo"): 
    y_bin = label_binarize(y_true, classes=classes) 
    n_classes = y_bin.shape[1] 
    fpr, tpr, roc_auc = {}, {}, {} 
    for i in range(n_classes):
        fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], y_score[:, i]) 
        roc_auc[i] = auc(fpr[i], tpr[i]) 
    fpr["micro"], tpr["micro"], _ = roc_curve(y_bin.ravel(), y_score.ravel()) 
    roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])
    # Plot 
    plt.figure(figsize=(8, 6)) 
    colors = cycle(['#1f77b4', '#ff7f0e', '#2ca02c'])   
    for i, color in zip(range(n_classes), colors): 
        plt.plot(fpr[i], tpr[i], color=color, lw=2, label=f'Classe {i} (AUC = {roc_auc[i]:.2f})')
    plt.plot(fpr["micro"], tpr["micro"], color='black', linestyle='--', 
    label=f"M√©dia micro (AUC = {roc_auc['micro']:.2f})") 
    plt.plot([0, 1], [0, 1], 'k--', lw=1) 
    plt.xlabel("Taxa de Falsos Positivos (FPR)") 
    plt.ylabel("Taxa de Verdadeiros Positivos (TPR)") 
    plt.title(f"Curva ROC Multiclasse ‚Äî {titulo}") 
    plt.legend(loc="lower right") 
    plt.grid(True) 
    plt.tight_layout() 
    plt.show()

# Fun√ß√£o de avalia√ß√£o completa 
def avaliar_modelo_classificacao(nome, y_true, y_pred, y_prob=None, classes=[0, 1, 2]): 
    # Matriz de confus√£o 
    cm = confusion_matrix(y_true, y_pred)  
    plt.figure(figsize=(5, 4)) 
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=['Baixa', 
    'M√©dia', 'Alta'], yticklabels=['Baixa', 'M√©dia', 'Alta']) 
    plt.xlabel("Predito") 
    plt.ylabel("Real") 
    plt.title(f"Matriz de Confus√£o ‚Äî {nome}", pad=12) 
    plt.tight_layout() 
    plt.show() 
    # M√©tricas textuais 
    print(f"\nAvalia√ß√£o ‚Äî {nome}") 
    print("Acur√°cia:", accuracy_score(y_true, y_pred)) 
    print("F1-score (macro):", f1_score(y_true, y_pred, average='macro')) 
    print("\nRelat√≥rio de Classifica√ß√£o:") 
    print(classification_report(y_true, y_pred, target_names=['Baixa', 
    'M√©dia', 'Alta'], zero_division=0))
    # Curva ROC (opcional) 
    if y_prob is not None: 
        plotar_curva_roc_multiclasse(y_true, y_prob, classes, titulo=nome)

def avaliar_modelos_classificacao():
    """Avalia os modelos de classifica√ß√£o"""
    # Previs√µes e probabilidades 
    y_pred_classico = modelo_classico.predict(X_test_class) 
    y_prob_classico = modelo_classico.predict_proba(X_test_class) 
    # Avalia√ß√£o completa 
    avaliar_modelo_classificacao("Modelo Sem PCA", y_test_class, y_pred_classico, y_prob_classico)
    
    # Previs√µes e probabilidades com PCA
    y_pred_pca = modelo_pca_class.predict(X_test_pca)
    y_prob_pca = modelo_pca_class.predict_proba(X_test_pca)
    # Avalia√ß√£o completa
    avaliar_modelo_classificacao("Modelo com PCA", y_test_class, y_pred_pca, y_prob_pca)

def plot_fronteira_decisao_2D(X_pca, y_true, modelo, titulo="Fronteiras de Decis√£o (PCA)"):
    h = 0.02 # Passo da malha
    
    # Gera√ß√£o da malha de pontos
    x_min, x_max = X_pca[:, 0].min() - 1, X_pca[:, 0].max() + 1
    y_min, y_max = X_pca[:, 1].min() - 1, X_pca[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))   
    # Predi√ß√£o sobre a malha
    Z = modelo.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    # Paleta de cores
    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])
    cmap_bold = ['red', 'green', 'blue']
    # Gr√°fico
    plt.figure(figsize=(8, 6))
    plt.contourf(xx, yy, Z, cmap=cmap_light, alpha=0.5)
    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_true,
    cmap=ListedColormap(cmap_bold), edgecolor='k', s=60)
    plt.xlabel("PC1")
    plt.ylabel("PC2")
    plt.title(titulo)
    plt.legend(handles=scatter.legend_elements()[0], labels=['Baixa','M√©dia', 'Alta'])
    plt.grid(True)
    plt.tight_layout()
    plt.show()

def visualizar_fronteiras_decisao():
    """Visualiza as fronteiras de decis√£o do modelo com PCA"""
    plot_fronteira_decisao_2D(X_class_pca, y_class, modelo_pca_class,
    titulo="Fronteiras de Decis√£o ‚Äî PCA + Regress√£o Log√≠stica")

def aprendizado_por_reforco():
    """Executa o algoritmo de aprendizado por refor√ßo Q-learning"""
    print("\n=== APRENDIZADO POR REFOR√áO (Q-LEARNING) ===\n")
    
    # Par√¢metros
    alpha = 0.9 # taxa de aprendizado
    gamma = 0.9 # fator de desconto
    epsilon = 0.9 # chance de explorar

    # Inicializa a Tabela Q
    q_table = {
    'muito_seco': {'muita_agua': 0.0, 'regar': 0.0, 'pouca_agua': 0.0, 'nao_regar': 0.0},
    'seco': {'muita_agua': 0.0, 'regar': 0.0, 'pouca_agua': 0.0, 'nao_regar': 0.0},
    'ideal': {'muita_agua': 0.0, 'regar': 0.0, 'pouca_agua': 0.0, 'nao_regar': 0.0},
    'encharcado': {'muita_agua': 0.0, 'regar': 0.0, 'pouca_agua': 0.0, 'nao_regar': 0.0},
    }

    # Fun√ß√£o de transi√ß√£o: pr√≥ximo estado baseado em estado atual e a√ß√£o
    def transicao(estado, acao):
        if estado == 'muito_seco':
            if acao == 'pouca_agua': return 'muito_seco'
            elif acao == 'regar': return 'seco'
            elif acao == 'muita_agua': return 'ideal'
            else: return 'muito_seco'
        elif estado == 'seco':
            if acao == 'regar': return 'ideal'
            elif acao == 'pouca_agua': return 'seco'
            elif acao == 'muita_agua': return 'encharcado'
            else: return 'seco'
        elif estado == 'ideal':
            if acao == 'regar': return 'encharcado'
            elif acao == 'pouca_agua': return 'ideal'
            elif acao == 'muita_agua': return 'encharcado'
            else: return 'seco'
        elif estado == 'encharcado':
            if acao == 'regar': return 'encharcado'
            elif acao == 'pouca_agua': return 'ideal'
            elif acao == 'muita_agua': return 'encharcado'
            else: return 'ideal'

    # x12= input("digite o modo 'extremo' ou 'padrao'")
    def recompensa(estado, acao, modo= 'extremo'):
        # Tabela de recompensas para o modo padr√£o
        padrao = {
            'muito_seco': {
                'regar': 3,
                'pouca_agua': 1,
                'nao_regar': -1,
                'muita_agua': 5
            },

            'seco': {
                'regar': 5,
                'pouca_agua': 2,
                'nao_regar': -1,
                'muita_agua': -3
            },
            'ideal': {
                'nao_regar': 5,
                'pouca_agua': 2,
                'regar': -3,
                'muita_agua': -5
            },
            'encharcado': {
                'nao_regar': 2,
                'pouca_agua': -1,
                'regar': -3,
                'muita_agua': -5
            }
        }

        # Tabela de recompensas para o modo extremo
        extremo = {
            'muito_seco': {
                'regar': 6,
                'pouca_agua': 0,
                'nao_regar': -3,
                'muita_agua': 8
            },

            'seco': {
                'regar': 8,
                'pouca_agua': 5,
                'nao_regar': -4,
                'muita_agua': -6
            },
            'ideal': {
                'nao_regar': -2,
                'pouca_agua': 4,
                'regar': 7,
                'muita_agua': -4
            },
            'encharcado': {
                'nao_regar': -4,
                'pouca_agua': -2,
                'regar': 3,
                'muita_agua': -6
            }
        }

        # Seleciona a tabela certa com base no modo
        tabelas = {
            'padrao': padrao,
            'extremo': extremo
        }

        if modo not in tabelas:
            raise ValueError("Modo inv√°lido. Use 'padrao' ou 'extremo'.")

        return tabelas[modo][estado].get(acao, -10)  # Penalidade extra se a√ß√£o inv√°lida

    # Registro para exibir evolu√ß√£o
    historico = []

    # Epis√≥dios de simula√ß√£o
    for episodio in range(1, 999):
        estado = random.choice(['muito_seco','seco', 'ideal', 'encharcado'])
        for passo in range(1): # Um passo por epis√≥dio (simplifica√ß√£o)
            if random.random() < epsilon:
                acao = random.choice(['muita_agua','regar', 'pouca_agua', 'nao_regar'])
            else:
                acao = max(q_table[estado], key=q_table[estado].get)
            prox_estado = transicao(estado, acao)
            r = recompensa(estado, acao)

            max_q_prox = max(q_table[prox_estado].values())
            q_atual = q_table[estado][acao]
            q_novo = q_atual + alpha * (r + gamma * max_q_prox - q_atual)
            q_table[estado][acao] = q_novo

            historico.append({
                'Epis√≥dio': episodio,
                'Estado': estado,
                'A√ß√£o': acao,
                'Recompensa': r,
                'Pr√≥ximo estado': prox_estado,
                'Q(s,a)': round(q_novo, 2)
            })
            estado = prox_estado # avan√ßa para o pr√≥ximo estado

    # Mostra a tabela final de Q-values
    q_df = pd.DataFrame(q_table).T
    print("\nTabela final de Q-values:")
    print(q_df)
    # Mostra hist√≥rico das decis√µes
    historico_df = pd.DataFrame(historico)
    print("\n√öltimas 10 decis√µes do hist√≥rico:")
    display(historico_df.tail(10))

def visualizacoes_regressao():
    """Menu para visualiza√ß√µes dos modelos de regress√£o"""
    print("\n=== VISUALIZA√á√ïES DE REGRESS√ÉO ===\n")
    
    opcoes = {
        '1': 'Scree Plot (An√°lise de Componentes Principais)',
        '2': 'Compara√ß√£o RMSE vs Lambda',
        '3': 'Compara√ß√£o de Modelos (temp_flor)',
        '4': 'Fun√ß√£o de Custo 1D',
        '5': 'Fun√ß√£o de Custo 2D',
        '6': 'Fun√ß√£o de Custo 2D com PCA',
        '7': 'Gr√°ficos de Res√≠duos'
    }
    
    print("Escolha uma visualiza√ß√£o:")
    for key, desc in opcoes.items():
        print(f"{key}. {desc}")
    print("0. Voltar ao menu principal")
    
    escolha = input("\nDigite sua escolha: ")
    
    if escolha == '0':
        return
    elif escolha == '1':
        screeplot()
    elif escolha == '2':
        visualizar_comparacao_lambda()
    elif escolha == '3':
        preparar_e_visualizar_modelos()
    elif escolha == '4':
        plot_funcao_custo_1D('temp_flor', X, y)
    elif escolha == '5':
        plot_funcao_custo_2D(['temp_flor', 'chuva_flor'], X, y)
    elif escolha == '6':
        plot_funcao_custo_2D_PCA(X_pca, y)
    elif escolha == '7':
        visualizar_residuos()
    else:
        print("Op√ß√£o inv√°lida!")

def executar_pipeline_completo():
    """Executa todo o pipeline de an√°lise"""
    print("\n=== EXECUTANDO PIPELINE COMPLETO ===\n")
    
    print("1. Carregando dados...")
    carregar_dados()
    
    print("\n2. Criando vari√°veis adicionais...")
    criar_variaveis_adicionais()
    
    print("\n3. Preparando dados para regress√£o...")
    preparar_dados_regressao()
    
    print("\n4. An√°lise de componentes principais...")
    screeplot()
    
    print("\n5. Treinando modelos de regress√£o...")
    treinar_modelos_regressao()
    
    print("\n6. Preparando visualiza√ß√µes de modelos...")
    preparar_e_visualizar_modelos()
    
    print("\n7. Preparando dados para classifica√ß√£o...")
    preparar_dados_classificacao()
    
    print("\n8. Treinando modelos de classifica√ß√£o...")
    treinar_modelos_classificacao()
    
    print("\n9. Avaliando modelos de classifica√ß√£o...")
    avaliar_modelos_classificacao()
    
    print("\n10. Visualizando fronteiras de decis√£o...")
    visualizar_fronteiras_decisao()
    
    print("\n11. Executando aprendizado por refor√ßo...")
    aprendizado_por_reforco()
    
    print("\n=== PIPELINE COMPLETO EXECUTADO COM SUCESSO! ===")

def menu_principal():
    """Menu principal do sistema"""
    while True:
        print("\n" + "="*50)
        print("SISTEMA DE AN√ÅLISE DE SAFRA COM APRENDIZADO POR REFOR√áO")
        print("="*50)
        print("\nMENU PRINCIPAL:")
        print("1. Carregar e preparar dados")
        print("2. An√°lise explorat√≥ria de dados")
        print("3. Modelos de regress√£o")
        print("4. Modelos de classifica√ß√£o")
        print("5. Aprendizado por refor√ßo (Q-Learning)")
        print("6. Visualiza√ß√µes de regress√£o")
        print("7. Executar pipeline completo")
        print("0. Sair")
        
        escolha = input("\nDigite sua escolha: ")
        
        if escolha == '0':
            print("\nEncerrando o programa...")
            break
        elif escolha == '1':
            carregar_dados()
            criar_variaveis_adicionais()
        elif escolha == '2':
            if df is None:
                print("\nPor favor, carregue os dados primeiro (op√ß√£o 1)")
            else:
                analise_exploratoria()
        elif escolha == '3':
            if df is None:
                print("\nPor favor, carregue os dados primeiro (op√ß√£o 1)")
            else:
                print("\n=== MODELOS DE REGRESS√ÉO ===")
                print("1. Preparar dados")
                print("2. Treinar modelos")
                print("3. Visualizar resultados")
                sub_escolha = input("\nDigite sua escolha: ")
                if sub_escolha == '1':
                    preparar_dados_regressao()
                    screeplot()
                elif sub_escolha == '2':
                    if X is None:
                        print("\nPor favor, prepare os dados primeiro")
                    else:
                        treinar_modelos_regressao()
                        preparar_e_visualizar_modelos()
                elif sub_escolha == '3':
                    if modelo_linear is None:
                        print("\nPor favor, treine os modelos primeiro")
                    else:
                        visualizacoes_regressao()
        elif escolha == '4':
            if df is None:
                print("\nPor favor, carregue os dados primeiro (op√ß√£o 1)")
            else:
                print("\n=== MODELOS DE CLASSIFICA√á√ÉO ===")
                print("1. Preparar dados")
                print("2. Treinar modelos")
                print("3. Avaliar modelos")
                print("4. Visualizar fronteiras de decis√£o")
                sub_escolha = input("\nDigite sua escolha: ")
                if sub_escolha == '1':
                    preparar_dados_classificacao()
                elif sub_escolha == '2':
                    if X_class is None:
                        print("\nPor favor, prepare os dados primeiro")
                    else:
                        treinar_modelos_classificacao()
                elif sub_escolha == '3':
                    if modelo_classico is None:
                        print("\nPor favor, treine os modelos primeiro")
                    else:
                        avaliar_modelos_classificacao()
                elif sub_escolha == '4':
                    if modelo_pca_class is None:
                        print("\nPor favor, treine os modelos primeiro")
                    else:
                        visualizar_fronteiras_decisao()
        elif escolha == '5':
            aprendizado_por_reforco()
        elif escolha == '6':
            if df is None or modelo_linear is None:
                print("\nPor favor, carregue os dados e treine os modelos de regress√£o primeiro")
            else:
                visualizacoes_regressao()
        elif escolha == '7':
            executar_pipeline_completo()
        else:
            print("\nOp√ß√£o inv√°lida! Tente novamente.")

# Executa o menu principal quando o script √© executado
if __name__ == "__main__":
    menu_principal()

#=======FEITO=======#
# 1. Altere as recompensas 
# a) Penalize mais o desperd√≠cio de √°gua. 

# b) Recompense melhor a√ß√µes que mant√™m o solo em estado ideal. 
#==========-----------------========================----------==#

# 2. Aumente o n√∫mero de epis√≥dios 
# a) Altere range(1, 51) para range(1, 201) e observe. linha 605
#  a a√ß√£o com maior valor na Tabela Q √© considerada a mais vantajosa, ou seja, a decis√£o preferida pelo agente.
#  ‚ñ™
#  A Tabela Q se estabiliza melhor? 
#   sim üëå
# O agente passa a evitar a√ß√µes ruins com mais consist√™ncia?
#   sim üëå

# 5. Altere as transi√ß√µes de estado 
# a) E se regar muito sempre levasse a encharcamento, mesmo quando o 
# solo estava ideal?